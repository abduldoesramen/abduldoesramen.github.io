# Week 1

_Claim_: All humans are mortal (to be True)
_Claim_: Socrates is a human (to be True)
_Deduced_: Therefore, Socrates is mortal.

Everything before "therefore" are **claims**.
Everything after "therefore" is a **conclusion**.
This style of proof is called an "**argument**".
Note: for this course: claims, statements and propositions can be used _interchangeably_.

An ethical argument, is an argument in which the conclusion is a **moral judgement/statement**.
A **moral judgement** is for example, "burning kittens is wrong".
_How can we know a moral argument (the reasons to believe moral/ethical claims) is right or wrong?_
This course will be exploring the different, major theories behind this idea.

### Utilitarianism

- **Utilitarianism** relies on making moral claims and ethical justifications based on imperical data.
- **Kantian ethics**, rely on pure axioms and does not rely on imperical data, i.e. pure computation, no external data.
- **Virtue ethics** is similar to data sets and machine learning

Utilitarianism is a variety of consequentialism: which is a theory of **normative** (as opposed to meta) ethics. **Consequentialism** is a theory of ethics that focuses on cause, e.g. causing suffering _is_ the wrong thing. Our actions do not have any intrinstic or inherent - moral values are the consequences of our actions that maximises happiness, minimise suffering. Utilitarianism schema are meant to apply universally all the time and is based on **naturalism** (alongside natural Sciences) that converts the bad and good to "measurable" suffering and happiness respectively. Cannot assign discrete values for borderline cases, but for vast majority of cases - we do know according to utilitarianism.

Real explainations are given by the natural sciences, need to be falsifiable based on evidence (from the scientific method). Hence utilitarianism allows ethical claims to be measurable from observing happiness and suffering. Normative moral claims give instructions. For example, 10 commandments are moral instructions.

Related to CompSci? Example: Trolley problem.

```
One-person in bottom-rail, none on the top-rail: save the one-person.
(Equally upset about dying, equal happy about living, qualitative the same).
One-person in bottom-rail, one in top-rail: Do nothing? Flip a coin?
Two-persons in bottom-rail, one in top-rail: Do nothing? "Play" God?
Five-persons in bottom-rail, one in top-rail: ?
Criminals? Innocent?: ?
Serial killer? Doctor?: ?
Good friend? Stranger?: ?
```

_Utilitarianism_: reduces interpersonal and moral obligations to mere calculations of utility. Would we ever have strong civilisations? Bias to certain groups has been the cause of terrible things. However arguably - it has also been the very thing that has allowed our species to flourish if it had not shown preferential bias.
Thus utilitariniasm: Suppose three-people bottom rail, one-people (dearest friend) on top rail. Pull the level and hit the friend.

The runaway trolley is a train that is moving autonomously, however you can move it. All self-driving cars are runaway rail carts with navigation protocols based on your choices on whether the switch should be pulled. Though, you are _one_ of the people on "track" because you are _on_ the vehicle. What if it was programmed by a utilitarianism? I.e. kills _you_ if it would possibly kill more than two people. Hence, what protocols should be in place when programming such autonomous vehicles?

Humans = unpredictable.
Vehicles = predictable.
For a modern example: 100,000 "runaway trolleys" - difficult to predict.

Modern day, due to the idea of consequentialism - major goal is to reduce suffering.
