<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT"
      crossorigin="anonymous"
    />
    <!-- Google tag (gtag.js) -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-DVN1VDSLVF"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-DVN1VDSLVF");
    </script>
    <title>COMP4920</title>
  </head>
  <body>
    <!-- Updated Navbar -->
    <nav class="navbar navbar-light sticky-top bg-light">
      <form class="container-fluid justify-content-start">
        <button
          type="button"
          onclick="location.href='https://abdulblog.me'"
          class="btn btn-light text-center btn-outline-success me-2"
        >
          About me
        </button>
        <button
          type="button"
          onclick="location.href='https://abdulblog.me/COMP3311.html'"
          class="btn btn-light text-center btn-outline-secondary me-2"
        >
          COMP3311
        </button>
        <button
          type="button"
          onclick="location.href='https://abdulblog.me/COMP4920.html'"
          class="btn btn-light text-center btn-outline-secondary me-2"
        >
          COMP4920
        </button>
        <button
          type="button"
          onclick="location.href='https://abdulblog.me/Mindmap.html'"
          class="btn btn-light text-center btn-outline-secondary me-2"
        >
          Mindmap
        </button>
      </form>
    </nav>
    <div class="alert alert-primary d-flex align-items-center" role="alert">
      I'll be adding notes to these tabs as the term progresses. Credit to
      "https://markdowntohtml.com/" for HTML code conversion from my MD file
      notes.
    </div>
    <h1 class="lead text-center display-2 text-muted">
      COMP4920 - Management and Ethics
    </h1>
    <!-- MD File CODE HERE -->
    <h1 id="summary-of-week01-lecture">Summary of Week01 Lecture</h1>
    <br />
    <p>
      <em>Claim</em>: All humans are mortal (to be True) <em>Claim</em>:
      Socrates is a human (to be True) <em>Deduced</em>: Therefore, Socrates is
      mortal.
    </p>
    <p>
      Everything before &quot;therefore&quot; are <strong>claims</strong>.
      Everything after &quot;therefore&quot; is a <strong>conclusion</strong>.
      This style of proof is called an &quot;<strong>argument</strong>&quot;.
      Note: for this course: claims, statements and propositions can be used
      <em>interchangeably</em>.
    </p>
    <p>
      An ethical argument, is an argument in which the conclusion is a
      <strong>moral judgement/statement</strong>. A
      <strong>moral judgement</strong> is for example, &quot;burning kittens is
      wrong&quot;.
      <em
        >How can we know a moral argument (the reasons to believe moral/ethical
        claims) is right or wrong?</em
      >
      This course will be exploring the different, major theories behind this
      idea.
    </p>
    <h3 id="utilitarianism">Utilitarianism</h3>
    <ul>
      <li>
        <strong>Utilitarianism</strong> relies on making moral claims and
        ethical justifications based on imperical data.
      </li>
      <li>
        <strong>Kantian ethics</strong>, rely on pure axioms and does not rely
        on imperical data, i.e. pure computation, no external data.
      </li>
      <li>
        <strong>Virtue ethics</strong> is similar to data sets and machine
        learning
      </li>
    </ul>
    <p>
      Utilitarianism is a variety of consequentialism: which is a theory of
      <strong>normative</strong> (as opposed to meta) ethics.
      <strong>Consequentialism</strong> is a theory of ethics that focuses on
      cause, e.g. causing suffering <em>is</em> the wrong thing. Our actions do
      not have any intrinstic or inherent - moral values are the consequences of
      our actions that maximises happiness, minimise suffering. Utilitarianism
      schema are meant to apply universally all the time and is based on
      <strong>naturalism</strong> (alongside natural Sciences) that converts the
      bad and good to &quot;measurable&quot; suffering and happiness
      respectively. Cannot assign discrete values for borderline cases, but for
      vast majority of cases - we do know according to utilitarianism.
    </p>
    <p>
      Real explainations are given by the natural sciences, need to be
      falsifiable based on evidence (from the scientific method). Hence
      utilitarianism allows ethical claims to be measurable from observing
      happiness and suffering. Normative moral claims give instructions. For
      example, 10 commandments are moral instructions.
    </p>
    <p>Related to CompSci? Example: Trolley problem.</p>
    <pre><code><span class="hljs-keyword">One</span>-person <span class="hljs-keyword">in</span> bottom-rail, none <span class="hljs-keyword">on</span> the top-rail: <span class="hljs-keyword">save</span> the <span class="hljs-keyword">one</span>-person.
    (Equally upset <span class="hljs-keyword">about</span> dying, equal happy <span class="hljs-keyword">about</span> living, qualitative the same).
    <span class="hljs-keyword">One</span>-person <span class="hljs-keyword">in</span> bottom-rail, <span class="hljs-keyword">one</span> <span class="hljs-keyword">in</span> top-rail: <span class="hljs-keyword">Do</span> nothing? Flip a coin?
    <span class="hljs-keyword">Two</span>-persons <span class="hljs-keyword">in</span> bottom-rail, <span class="hljs-keyword">one</span> <span class="hljs-keyword">in</span> top-rail: <span class="hljs-keyword">Do</span> nothing? <span class="hljs-string">"Play"</span> God?
    Five-persons <span class="hljs-keyword">in</span> bottom-rail, <span class="hljs-keyword">one</span> <span class="hljs-keyword">in</span> top-rail: ?
    Criminals? Innocent?: ?
    Serial killer? Doctor?: ?
    Good friend? Stranger?: ?
    </code></pre>
    <p>
      <em>Utilitarianism</em>: reduces interpersonal and moral obligations to
      mere calculations of utility. Would we ever have strong civilisations?
      Bias to certain groups has been the cause of terrible things. However
      arguably - it has also been the very thing that has allowed our species to
      flourish if it had not shown preferential bias. Thus utilitariniasm:
      Suppose three-people bottom rail, one-people (dearest friend) on top rail.
      Pull the level and hit the friend.
    </p>
    <p>
      The runaway trolley is a train that is moving autonomously, however you
      can move it. All self-driving cars are runaway rail carts with navigation
      protocols based on your choices on whether the switch should be pulled.
      Though, you are <em>one</em> of the people on &quot;track&quot; because
      you are <em>on</em> the vehicle. What if it was programmed by a
      utilitarianism? I.e. kills <em>you</em> if it would possibly kill more
      than two people. Hence, what protocols should be in place when programming
      such autonomous vehicles?
    </p>
    <p>
      Humans = unpredictable. Vehicles = predictable. For a modern example:
      100,000 &quot;runaway trolleys&quot; - difficult to predict.
    </p>
    <p>
      Modern day, due to the idea of consequentialism - major goal is to reduce
      suffering.
    </p>
    <h1 id="summary-of-week02-lecture">Summary of Week02 Lecture</h1>
    <br />
    <h2 id="major-three-types-of-normatic-ethics-">
      Major three types of Normatic Ethics:
    </h2>
    <br />
    <h3 id="utilitarianism-consequentialism">
      Utilitarianism/Consequentialism
    </h3>
    <p>
      Ethics and moral judgement is achieved through the measurement
      (observation) and maximisation of utility (happiness over sadness) based
      on imperical evidence.
    </p>
    <h3 id="kantian-ethics">Kantian Ethics</h3>
    <p>
      Ethics and moral judgement come within through the exploration of
      categorical imperative (universality of certain laws) and does not rely on
      any imperical data.
    </p>
    <h3 id="virtue-ethics">Virtue Ethics</h3>
    <p>
      Ethics and moral judgement are derived through one&#39;s virtues and
      emotions through the comparison of a more virtuous individual.
    </p>
    <h3 id="tutorial-content">Tutorial Content</h3>
    <h4 id="exploring-deontology-">Exploring Deontology:</h4>
    <p>Util. &lt;---&gt; Deontology.</p>
    <p>Rule-based Util (within the middle).</p>
    <p>
      Deontology strongly prohibits certain classes of actions (that fails
      categorical imperative test).
    </p>
    <p>
      Negative duty (for example: Don&#39;t murder, your duty is to NOT murder
      -&gt; which is a perfect duty). (Passes imperative test): Permissable (for
      example: assisting the poor, upto you how much and when -&gt; which is an
      imperfect duty). NOT empirical, more axiomatic (conceptual) rather than
      observation, Kant analysis concepts/definitions and extrapolates from such
      concepts.
    </p>
    <p>Deontologist (rule-based theory):</p>
    <ul>
      <li>Kant is worried about excuses, and justifications of the wrong.</li>
      <li>
        Contemporary deontologist are worried about horrible actions being
        justified.
      </li>
    </ul>
    <p>
      Empirical = a posteriori (we can&#39;t do anything until post
      observation). Conceptual = a priori (don&#39;t need to observe anything,
      it is prior observation).
    </p>
    <p>Preliminary works:</p>
    <ul>
      <li>
        (Relevant)
        <em
          ><strong>Ground work</strong> (the beginning) to metaphysics of
          morals</em
        >, written in 1785. Based on the human capacity for free agency or
        autonomy (Auto - self, Nomis - law: give the law to ourselves), the
        human being is different from instinct-driven animals. Animals cannot
        make deep considerations of the future (lack long-term planning). Human
        beings can direct their lives according to reason. Capacity of autonomy
        Kant says is very important, humans have a duty (thinking of their
        actions and consequences) to animals but an animal cannot have a duty to
        humans. Kant argues that autonomy gives us dignity - that we can live
        our lives in the &quot;long-term&quot; and should only determined by
        ourselves - we do not need a third-party (any individual/government) to
        decide for us. Thus implies that humans are Ends (goal/outcome) in
        Themselves (you create your own end/future through rational planning
        <em>for</em> ourselves).
        <ol>
          <li>
            <strong>Maxim: general rule.</strong> (Example: give to the poor, do
            not steal, muder etc.) Could I desire this to be a universal law?
            For example: give to the poor, not self contradicting because no one
            WANTS to be poor. Borrowing money, but not giving back but now
            society will never trust anyone with their money.
            <strong
              >Maxim: it&#39;s okay to borrow money, but not give back</strong
            >.
          </li>
          <li>
            There would be times there would be exceptions to these universal
            rules for example: lying to save a life, this exemption case is now
            a universal case that will pass, allowing for exceptions. However
            now Deontology is moving towards the middle, where rule
            utilitarianism is near (similar to how introducing rules to
            Utilitarianism moves towards rule Util.).
          </li>
        </ol>
      </li>
      <li><em>Critique of practical reasoning</em>, written in 1788.</li>
    </ul>
    <p>Not <em>much</em> impact:</p>
    <ul>
      <li><em>Metaphysics of Morals</em>, written in 1797.</li>
    </ul>
    <h4 id="exploring-virtue-theory-">Exploring Virtue Theory:</h4>
    <p>Aristotle Virtue Theory:</p>
    <ul>
      <li>Virtue Theory is not based on decision-making.</li>
      <li>
        Majority of history is about the character/judgement of others and
        one&#39;s self.
      </li>
      <li>
        Aristotle wanted to create a well-rounded theory behind virtues, rather
        than the virtue of aggression and domination (single virtues) from Greek
        history. Rather wanting to produce a character in rationality, a
        character that is more well-rounded (multiple virtues).
      </li>
    </ul>
    <p>Contemporary Virtue Theory:</p>
    <ul>
      <li>
        Utilitarianism and Kantian ethics seems inhumane, for example: Harry S.
        Truman using utilitarianism to justify the bombing of Japan.
      </li>
      <li>
        General motivation: there is too much left (that make us human) out from
        the two theories, for example utilitarianism ruling out human intimate
        relationships when calculating utility (intimacy makes us human). The
        thought that a moral theory would throw out what a human is repulsive,
        essentially asking us to become in-human. There&#39;s no space for love,
        building relationship etc.
      </li>
      <li>
        Kant&#39;s theory is shown as very narrowly focused (universal laws,
        things you must NOT do).
      </li>
      <li>
        Other two theories are too technical and not based on human
        emotions/experience, such as love.
      </li>
    </ul>
    <p>Back to Aristotle Virtue Theory...</p>
    <ul>
      <li>
        Often Virtue Ethicists are critical of utilitarianism and deontologists.
      </li>
      <li>
        Often is associated on <em>how</em> we live our lives, and focuses on
        <em>becoming</em> virtuous.
      </li>
      <li>
        <p>
          Aristotle: You will never be able to generate a system of rules that
          will <em>always</em> be correct in every situation because moral
          problems are multidimensional.
        </p>
        <ol>
          <li>
            <p>
              However you <em>can</em> grow and <em>improve</em> yourself into a
              better human being that is more sensitive to moral dimensions of
              life (similar to how 10,000 hours is seen as an expert).
            </p>
          </li>
          <li>
            <p>
              Essentially, since the more you think, practice and engage about
              moral theory/virtue, you will become more sensitive to the moral
              dimensions of life.
            </p>
          </li>
          <li>
            <p>
              Develop a habit: surround a man with other virtuous men, for
              example: practising medicine around experts, learning about the
              theory behind driving but never stepping inside a vehicle.
            </p>
          </li>
        </ol>
      </li>
      <li>
        <p>
          Agent centered, but how does this relate to others? Others can benefit
          from one&#39;s virtuous activity, for example: mutual benefits from
          one&#39;s profession - especially in Information Technology (IT).
        </p>
      </li>
      <li>
        Virtues are <em>character traits</em>, for example: courage, patience,
        justice, fairness etc.
      </li>
      <li>
        <p>
          However Aristotle sees one character traits above others: prudence.
        </p>
        <ol>
          <li>
            <p>
              Phronesis (balancing the virtues), for example: Courage might get
              in the way of temperance. Virtues requires wisdom and regulation
              so they do not conflict and undermine one another.
            </p>
          </li>
          <li><p>Prudence is often known as practical wisdom.</p></li>
          <li>
            We accept virtues have the capacity of clashing. Prudence is the
            capacity to see on how to balance one&#39;s virtues, for example:
            giving a friend money when they are a drug addict.
          </li>
          <li>
            Prudencia takes wisdom (<em>insight</em>) to solve certain moral
            problems.
          </li>
        </ol>
      </li>
      <li>
        <p>
          <strong>Overall goal:</strong> Eudaimonia (good | spirt), be
          good-spirited and to do well, flourish.
        </p>
        <ol>
          <li>
            <p>Often translated to <em>flourishing</em>.</p>
          </li>
          <li>
            <p>
              For example: physically well, exercising their mind to be
              intelligent, thoughtful, accumalated enough things needed, have
              enough skill in their career, good relationship etc. Essentially
              doing well overall (flourishing across multidimensions).
            </p>
          </li>
        </ol>
      </li>
    </ul>
    <p>
      Artistotle has a functional concept of good (moral conception: saints,
      agents). However Artistotle believes that a thing is good if it serves its
      purpose <em>well</em>, for example: a knife is sharp and cuts well, a good
      race horse is that one that is fast. However a function of a human being
      is complicated, but is based on rationality and well-being but there is
      more to that. For example:
    </p>
    <pre><code>
        <span class="hljs-number">1</span>. Good doctor: Virtues <span class="hljs-keyword">of</span> a doctor.
        <span class="hljs-number">2</span>. Good soldier: Virtues <span class="hljs-keyword">of</span> a soldier.
        <span class="hljs-number">3</span>. Good journalist: Virtues <span class="hljs-keyword">of</span> a journalist.
    </code></pre>
    <p>
      Virtues are virtuous because they help us function. Emotions = Non
      Rational, however Aristotle seems to think that that our emotions have
      registered/seen something important, i.e. alert us.
    </p>
    <p>
      Virtues are caught between vices. For example, overly scared (cowardly),
      not scared enough (fool). Courage lies inside in the middle, i.e the
      <em>golden mean</em>. Either side leads to a failure to process this
      event. Aristotle: Virtues --&gt; Rationality of Passion --&gt; Golden
      Mean: between excess and deficiency.
    </p>

    <!-- WEEK 3 -->

    <h1 id="summary-of-week03-lecture">Summary of Week03 Lecture</h1>
    <h3 id="ethics-and-law">Ethics and Law</h3>
    <p>General message:</p>
    <pre><code>Legal != Ethical
Ethical != Legal
</code></pre>
    <p>
      For example, Legal != Ethical: Queensland laws can accuse Women of
      Witchcraft. Ethical != Legal: Jumping queues when waiting for your coffee.
      There is no strict one-to-one relationship between ethics and law, but
      your code of ethics/conduct is going to be shaped by our laws.
    </p>
    <p>
      Laws are generally supposed to be &quot;Action Guiding&quot;, for example:
      speeding - how fast you should be going. How does ethic guide your
      conduct? For example: ACM Code, breach of Code leads to expulsion ACM is
      important for reputation on your professional, for example: doctors
      associated with college of physicians.
    </p>
    <h3 id="values-principles-practices-and-purpose">
      Values, Principles, Practices and Purpose
    </h3>
    <p>
      Values = What we strive for, for example: value of Integrity,
      Transparency, etc. Principles = Shape conduct, for example: common
      principles in professional ethics: the harm minimisation principle, do no
      harm or justice: which equates to fairness, beneficious: which equates to
      social. Responsibility = For any principle or professional value, we are
      responsible and expectated to upheld them. Practices = How we follow and
      implement such principles or professionals in the real world. Purposes =
      How we unite Values and Principles, and why we list/follow such code of
      ethics/conduct.
    </p>
    <h3 id="is-technology-neutral-">Is Technology Neutral?</h3>
    <p>
      Perspective 1: (law 1) Technology is not good nor bad, and not neutral.
      For example: &quot;Guns don&#39;t kill people, people do&quot;,
      technically true but guns improve efficiency in killing others over a
      knife. &quot;Military technology&quot; originated day-to-day technology.
      &quot;Atomic Bomb&quot;: Purpose is to mass kill, indiscriminate including
      civilian targets, radioactive fallout, and is not precise in killing
      unlike an assault rifle, however is an atomic bomb good/bad/neutral? What
      exactly is the &quot;good&quot; of atomic bombs? Ending war, however more
      countries uphold it. Technology gives options, however certain options do
      not generally have a good outcome.
    </p>
    <p>
      (law 2) Necessity is the mother of innovation For example: privacy and
      confidentiality - users were not thinking of data security and privacy.
      United Nations: exploring breach of human rights via digital means, 2023
      summit occuring which will eventually influence Australian technoly-based
      law.
    </p>
    <p>
      <strong>Technology is a means with external ends</strong>: The use we put
      technology to ultimately depends on the user, for example: assault rifle
      has no say on how it is used. The <strong>way</strong> we use technology
      can be either good or bad.
    </p>
    <h3 id="history-of-technology-is-relevant">
      History of technology is relevant
    </h3>
    <p>Important inventions:</p>
    <p>
      Basic form of technology: writting/script - preserve information. Modern
      form of technology: smartphone - numbers/information conveyed through
      script.
    </p>
    <p>
      Printing press: Prior to the printing press, 95% of the population could
      not read, movement of oral to written-text based culture. Allowed mass
      printing and circulation of books and literature. Movement from the
      medieval world to the modern.
    </p>
    <p>
      Armour: military-based technology. Communication via telegram
      (revolutionary). Modern: smartphone, circuits, electricity.
    </p>
    <p>
      Marxist history is based on innovations from technology, society is based
      on such progression.
    </p>
    <h3 id="responsibility-from-the-creators-of-technology">
      Responsibility from the Creators of Technology
    </h3>
    <ul>
      <li>
        Researchers make decisions, and any decision made can have an ethical
        aspect behind it, for example: inventor of the Atomic Bomb.
      </li>
      <li>
        Think in terms of ethics, and the outcome when inventing
        products/services.
      </li>
      <li>
        What is the use potential of what I am attempting to invent. How could a
        malicious user of this product abuse it? What is the worst-case use?
      </li>
      <li>Invent some sort of buffer to prevent abuse of products/services.</li>
      <li>
        For example: Designers of the internet were not actively thinking of the
        sharing of abusive material when inventing the internet, the bitcoin was
        propped by the drug-market from the demand of having a decentralised and
        deregulated currency.
      </li>
    </ul>
    <h3 id="technology-opens-possibilities">Technology opens possibilities</h3>
    <ul>
      <li>Molly-Russell Case: exposure to suicide-type content.</li>
      <li>
        Young girl suffering depression using instagram to browse content
        related to self-harm and suicide, moving into areas where other
        depressed users were sharing content.
      </li>
      <li>
        Instagram: how to prevent? Social media is a vehicle to express
        themselves, e.g. happy people share joyful content, depressing/suicidal
        users will express themselves in suicidal ways.
      </li>
      <li>
        Reinforce suicidal thinking on their pysche, desires, life-style and
        health.
      </li>
      <li>
        Understand that there are a range of expressions that can be expressed
        on Social Media based on human nature.
      </li>
      <li>
        Result: Instagram executives acknowledges <em>some</em> responsibility
        to the Molly-Russell case.
      </li>
    </ul>
    <h3 id="code-of-conduct">Code of Conduct</h3>
    <p><strong>Foundation:</strong></p>
    <p>
      ACM Code: Developed through an ethical approach of principlism. 1979:
      Beauchump &amp; Childres - responding to the idea of high-moral theories
      are difficult for average people to employ. Thinking primarily in medical
      ethics, &quot;4 Principles of Bio-ethics&quot; sets the foundation of
      profession ethics and code of ethics/conduct in the 1980s onward.
    </p>
    <ul>
      <li>Beneficiance = do good.</li>
      <li>Non-Meleficence = do no harm.</li>
      <li>Autonomy = self direction</li>
      <li>Justice = fairness</li>
    </ul>
    <p>
      Attempted to take the main moral theories into principles. Specific nature
      of cases (ethical problems), we look at the case, think of moral
      implications/our intuitions, how we resolve other similar cases, and look
      at the principles. Bring these three things into equilibrium with each
      other and develop such principles further. Hence there is expanding
      principles. Each individual principle may develop (understand them better)
      and principles are added.
    </p>
    <p>
      1 section: Principles 2 section: Professional responsibilities 3 section:
      Professional leadership 4 section: potential expulsion if not following
      the ACM.
    </p>
    <h3 id="breakdown-of-principles-in-the-code">
      Breakdown of Principles in the Code
    </h3>
    <p>Point 1.1:</p>
    <ul>
      <li>Contribute to Wellbeing (util)</li>
      <li>All People are Stakeholders (util)</li>
    </ul>
    <p>Point 1.2:</p>
    <ul>
      <li>Negative consequences (util)</li>
      <li>Unjustified Harms (principle)</li>
      <li>Well intentional acts that lead to harm (principle)</li>
    </ul>
    <p>Point 1.3:</p>
    <ul>
      <li>Transparency (virtue)</li>
      <li>No false claims (Deontology)</li>
      <li>Honesty about Qualifications (virtue)</li>
      <li>Conscious of Conflicts (virtue)</li>
    </ul>
    <p>Point 1.4:</p>
    <ul>
      <li>Non-discrimination (Deontology)</li>
      <li>Fairness and allowing for redress (Deontological + Principilist)</li>
      <li>
        Limiting inequality, i.e. Universal Access (virtue + Deontological)
      </li>
    </ul>
    <p>Point 1.5:</p>
    <ul>
      <li>Intellectual property rights (Deontological)</li>
      <li>Need for Sharing Work and Cooperation (virtue)</li>
    </ul>
    <p>Point 1.6:</p>
    <ul>
      <li>Individual rights to privacy (Deontological)</li>
      <li>Issue of Consent</li>
    </ul>
    <p>Point 1.7:</p>
    <ul>
      <li>Entrusted (virtue, i.e. trustworthy)</li>
      <li>Honour confidentiality (virtue) (except if it breaks the law)</li>
    </ul>
    <p>
      <em>Court</em>: Duty to care, Was this Duty to Care Breached, is there
      negative Consequences as a result? <em>ACM Code</em>: Act according to ACM
      Code.
    </p>
    <h3 id="consent">Consent</h3>
    <ol>
      <li>Capacity (ability to cognitively make a decision)</li>
      <li>Information (informed)</li>
    </ol>
    <h3 id="dignity">Dignity</h3>
    <p>
      Your Code derives dignity, not from Kant but from the
      <strong>Universal Decleration of Human Rights</strong>. However the UDHR
      was written by Hans Kelson who is a Kantian but Kelson also uses
      <em>religious conceptions</em> of dignity and <em>Ancient Rome</em>.
    </p>
    <p>
      (note: Rights and Duties, 2 sides 1 coin - and generally refers to
      Deontology)
    </p>

    <!-- WEEK 4 -->

    <h1 id="summary-of-week04-lecture">Summary of Week04 Lecture</h1>
    <p>
      All credit goes to UNSW COMP4920&#39;s notes, I do not own a part of these
      notes - they have been inspired from my Tutor&#39;s work: Philip Quadrio,
      12/10/2022.
    </p>
    <h3 id="ethics-washing">Ethics Washing</h3>
    <p>
      Ethics washing refers to specific critique, differing for Munn&#39;s
      central criticism. For example, airline companies promoting a green image
      despite being in a polluting industry.
    </p>
    <h3 id="munn-ai-ethics-has-a-capacity-to-harm">
      MUNN: AI Ethics has a capacity to harm
    </h3>
    <ul>
      <li>
        Harm is more than physical harm - psychology, emotional, property.
      </li>
      <li>
        Can exacerbate inequality - racial or demographic profiling, impacting
        on minorities and worst off.
      </li>
      <li>Racialised assumptions - in security, in screening (jobs) ...</li>
      <li>
        Undermines Democratic values - social media disinformation during
        electoral cycle
      </li>
      <li>Human Rights and dignity - rights to privacy</li>
      <li>
        AI Ethics <em>social space</em> that is already stratified (economic,
        racial and gendered stratification)
      </li>
    </ul>
    <p>
      For example: Parallels to Townhall&#39;s old CCTV scripting to find
      middle-eastern men, young men in hoodies etc. Social media - impacting US
      Federal elections, Russia causing unrest via social media for both the
      left and right. Pitch products to consumer habits lead to lack of privacy.
    </p>
    <h3 id="munn-turn-to-ai-ethics-ineffective">
      MUNN: Turn to AI ethics ineffective
    </h3>
    <ul>
      <li>
        AI Ethics: in Munn&#39;s view - is about <em>regulation</em> through
        codes and principles.
      </li>
      <li>
        Codes and Principles seem to <em>neglect</em> the political, social and
        cultural dimensions bound up with AI in general.
      </li>
      <li>
        While we fail to address the broader political, social and cultural
        dimensions it will continue to harm.
      </li>
      <li>
        Codes work through principles, practices are shaped by politics,
        economics and culture.
      </li>
      <li>
        So, <strong>Gap between Principles and Practice</strong> as Practice is
        shaped by politics, economics and culture. However priciples simply sit
        somewhere.
      </li>
    </ul>
    <p>
      AI production is lead from deeper problems inside society and is simply
      layered ontop of these deeper issues inside society. Thus, source of the
      &quot;harm&quot; will remain.
    </p>
    <h3 id="munn-meaningless-principles">MUNN: Meaningless Principles</h3>
    <ul>
      <li>Meaningless, isolated and toothless</li>
      <li>
        Highly abstract and ambigious principles which
        <em>almost become incoherent</em> for example: moral philosophy = the
        &#39;good&#39; = about beneficence = do &#39;good&#39;(?).
      </li>
      <li>
        They promise to be action guiding but are ultimately too abstract to
        guide practical actions.
      </li>
      <li>They provide few specific recommendations.</li>
      <li>
        Concepts such as Autonomy, Beneficence, Non-Maleficence, Justice and
        Explicability are contested, that is; the principles are expressed
        through concepts that can be interpreted differently, contesting
        meaning.
      </li>
      <li>
        There can be conflicts and contradictions between them sometimes
        respecting autonomy contradicts other principles (doing good -
        beneficence).
      </li>
      <li>
        They can be defined to suit a corporate agenda that might not be good
        for society/humanity.
      </li>
      <li>Fail to address political tension.</li>
    </ul>
    <p>
      For example: these concepts can have differing perspectives/definitions
      based on who ask. Principles can also contradict with each other - for
      example: an addicted drug-user in rehab. Furthermore, might cater to a
      corporate agenda that may ultimately be negative to society. case-example:
      not using plastics from airline companies but still uses fuel... shapes
      and twists principles.
    </p>
    <h3 id="munn-isolated-principles">MUNN: Isolated Principles</h3>
    <ul>
      <li>&quot;The principles are isolated.&quot;</li>
      <li>
        AI Ethics discretely parceled up and tucked away as a niche topic at end
        of a course of study, not integrated into the wider program of study.
      </li>
      <li>
        Development of tech occurs in a social and political context: it is
        social, cultural and political and is influence by society, culture and
        political practices as well as existing tech practices.
      </li>
      <li>
        Education focus is on professionalism and professional ethics and
        ignores social and political issues.
      </li>
      <li>
        The toxic culture (e.g. Brogrammers) is bound up with broader cultural,
        social and political issues.
      </li>
      <li>
        <strong>Isolated</strong> from social and political background leads to
        <em>&#39;tweaking&#39;</em> as a remedy (but this tweaking is too
        narrow),
      </li>
      <li>
        Organisational culture and commerical culture leads to a focus on
        &#39;ethics&#39; and ineffectual solutions.
      </li>
      <li>
        Failure to build ethical AI is an organisational failure, to address the
        <em>sources</em>.
      </li>
    </ul>
    <p>
      Brogrammer: hypermasculine programmer, for example: midnight, shirtless,
      sunglasses etc. This is bound by broader societal, political and cultural
      issues, stems from &#39;Bro&#39; which is created from the masculine
      culture in society. Hence this &#39;AI problem&#39; stems from a deeper
      issue.
    </p>
    <h3 id="munn-toothless-principles">MUNN: Toothless Principles</h3>
    <ul>
      <li>
        There is nothing to enforce these principles, However Code has expulsion
        as a consequence. Note: AMA (aus med assoc) conveys doctor has been
        deregisted. Big impact on doctor&#39;s career/life.
        <ol>
          <li>
            ACM does not seem to have direct communication of expelled workers.
          </li>
          <li>
            Question: how much will this disciplinary action push a worker to
            reflect on their own actions?
          </li>
        </ol>
      </li>
      <li>
        The principles <em>lack enforcement measures</em> - perhaps expulsion
        from ACM.
      </li>
      <li>
        <em>Incentive to enforcement overwritten by economic incentives.</em>
      </li>
      <li>
        Companies will <strong>actively</strong> impede, resist, reinterpret and
        overturn governance regulations (for <em>profitability</em>).
        <ol>
          <li>
            In regard to State and Federal law they will lobby government for
            self regulation or soft regulation.
          </li>
          <li>
            Often insist that &#39;market solutions&#39; are best solution (fits
            with a commercial and capitalist culture).
          </li>
        </ol>
      </li>
      <li>
        MUNN: What is the point of the principles and instructing on the
        principles if there is <strong>not</strong> tangible enforcement?
        <ol>
          <li>
            Possible response: move away from industry-based regulations (ACM)
            towards more government based regulation but industries will
            actively resist/lobby.
          </li>
          <li>
            For example: APRA overturns AMA (from industry-based to government),
            leading to more formal/heavy disciplinary action.
          </li>
        </ol>
      </li>
    </ul>
    <h3 id="beitti-ethics-washing-corporate-fascade">
      BEITTI: Ethics washing = corporate fascade
    </h3>
    <ul>
      <li>Ethics washing is <em>never fundamentally</em> about ethics.</li>
      <li>
        <strong>Appearances vs reality</strong>: Ethics washing is about
        organisations &#39;appearing&#39; ethical not &#39;being&#39; ethical.
      </li>
      <li>
        Instrumentalises ethics language, that is, uses &#39;ethics&#39; speak
        to facilitate non-ethical ends.
        <ol>
          <li>Appearing ethical is good for the bottom line.</li>
          <li>Appearing ethical is good for reputation.</li>
          <li>
            Appearing ethical can create a good contrast with competitors.
          </li>
        </ol>
      </li>
      <li>
        Ethics washing <em>simplifies</em> ethical work.
        <ul>
          <li>
            Companies do not care about the complexity of moral ethical thought,
            they simply pursue a reputation to &#39;appear&#39; ethical.
          </li>
        </ul>
      </li>
      <li>
        The happy side of AI ethics leads us to think deregulation or
        self-regulation is justified, it could lead us to prefer market
        solutions - but this benefits organisations more than society.
      </li>
    </ul>
    <p>
      Comment: public charity giving on huge check infront of media in a
      townhall setting, companies name is shown and amount of money, seems to be
      reputation-based. Question: If you are not allowed to create massive media
      attention from the act of charity, would you give the money in the first
      place?
    </p>
    <p>YOU vs Competitor: being &#39;ethical&#39; can improve bottom-line.</p>
    <h3 id="beitti-instrumental-vs-intrinsic">
      BEITTI: Instrumental vs. Intrinsic
    </h3>
    <ul>
      <li>
        If we note that ethics washing instrumentalises the language of ethics
        then we can make this distinction.
      </li>
      <li>
        Intrinsic value of ethical language: ethics is valuable in of itself,
        independently of any end it serves.
        <ol>
          <li>
            This is a real commitment to ethics and a real attempt to help us be
            morally conscientious agents.
          </li>
          <li>
            Ethics and moral philosophy helps us understand and appreciate the
            complexity of moral problems leading to more sophisticated and
            nuanced answers to those problems.
          </li>
        </ol>
      </li>
      <li>
        Instrumental value of ethical language: the goal and ends of ethical or
        moral talk are either secondary or superfluous to the &#39;real&#39;.
        object of ethical talk = organisational and corporate goals and ends.
        <ol>
          <li>
            Ethics and moral philosophy just a tool used to achieve non-ethical
            goals or ends.
          </li>
          <li>
            NOTE: instrumentalism - the ends are complexity external to means
            (goals external to &#39;ethical tools&#39;).
          </li>
        </ol>
      </li>
      <li>
        Related to MUNN: When ethics is motivated bv goals to external to moral
        philosophy, the more doubtful we can about its outcomes for society.
      </li>
    </ul>
    <h3 id="beitti-narrowness-of-ethics">BEITTI: Narrowness of Ethics</h3>
    <ul>
      <li>
        Instrumentalised ethics has a narrow focus because the ethics ends are
        not the real goals.
      </li>
      <li>
        The narrower the focus is on a problem, the more narrow the response to
        that problem.
        <ul>
          <li>
            E.G. tweaking product for gender-fariness, miss deeper
            organisation/industry/culture issue.
          </li>
          <li>
            Racial basis - if we narrow to just fairness (tweaking AI), we miss
            the more systemic issue of race, the racialised assumptions of
            programmers, or of the organisation, industry and culture.
          </li>
        </ul>
      </li>
    </ul>
    <h3 id="embedding-moral-philosophers-in-organisations">
      Embedding moral philosophers in organisations
    </h3>
    <ul>
      <li>
        Seems a positive move: appears to have a more robout moral and
        philosophical conversation.
        <ul>
          <li>
            BEITTI agrees: need to acknowledge that sometimes self regulation is
            better/more efficient.
          </li>
          <li>A desire to foster a positive image is understandable.</li>
        </ul>
      </li>
      <li>
        Could serve corporations <em>more</em> than users.
        <ul>
          <li>Should serve humanity over companies interest but may not.</li>
          <li>
            Can be used as another element in another ethic washing project.
          </li>
        </ul>
      </li>
      <li>
        (To ONLY use a philosopher onboard) Could shield the company from
        external regulation while continuing unethical practices that are
        profitable.
      </li>
    </ul>
    <h3 id="beitti-ethics-bashing">BEITTI: Ethics Bashing</h3>
    <ul>
      <li>Biproduct of ethics washing.</li>
      <li>
        <em>Mostly</em> comes from sociologists and non-philosphers who are
        concerned by the <strong>instrumental</strong> use of ethics language.
        They see how it serves <em>corporate ends</em>.
      </li>
      <li>
        It conflates the term &#39;ethics&#39; with what is encountered in the
        practice of the ethics bv companies engaged in washing: so... they
        understand ethics through the <em>lens of ethics washing</em>.
      </li>
      <li>
        Ethics becomes <em>reduced</em> to ethics washing and all ethical talk
        is viewed as a &#39;corporate speak&#39; that
        <em>conceals a non-ethical agenda</em>.
      </li>
      <li>
        So, ethics is bashed because &#39;ethics itself&#39; is just a mask for
        something else.
        <ul>
          <li>
            Could be related to Marxist critique... clouding the pursuit of
            profit with ethical/mystic language.
          </li>
        </ul>
      </li>
    </ul>
    <p>
      E.g. LGBTQ Pride, Dell and AT&amp;T showcasing pride logo but donating to
      anti-lgbt based politicians. These corporate brandings do not represent
      any ethical/moral standing, and may exist to only appeal to the public to
      appease corporate agenda.
    </p>

    <!-- WEEK 5 -->
    <h1 id="summary-of-week05-lecture">Summary of Week05 Lecture</h1>
    <h3 id="stakeholder-theory">Stakeholder Theory</h3>
    <p>
      Used to have Shareholders/Stockholders which are basically owners. However
      this approach has fallen out of favour, we tend to think of management as
      &quot;Stakeholders&quot;. Stakeholders can either be <em>direct</em> (e.g
      End-Users, Employees, Clients, Investors, Suppliers) or
      <em>indirect</em> (e.g Community, Regulators, Government).
    </p>
    <h3 id="comprehensive-moral-responsibility">
      Comprehensive Moral Responsibility
    </h3>
    <p>
      Milton: only moral responsibility is on shareholders/stockholders.
      Stakeholder Theory: all stakeholders have moral responsibility.
      Comprehensive Moral Responsibility: impacts from product/service on
      humanity/environment, i.e. broadening of interest.
    </p>
    <h3 id="-article-responsible-autonomy">(Article) Responsible Autonomy</h3>
    <p>
      Autonomy: self-guidance/<em>self-governence</em> Responsibility:
      answerable/blameworthy Responsible Autonomy: connects the notion of
      autonomous (AI, machines) to notions of moral responsibility. There is a
      <em>need</em> for a responsible approach to AI.
    </p>
    <ul>
      <li>Ethical: how they impact on people/environment</li>
      <li>
        Legal Consequences: national + state legislation and possibility to
        extend to <em>Australian Human Rights Commission</em> (search with
        keyword AI for presentation).
      </li>
    </ul>
    <p>
      There needs to be a design for tools and methods in order to reflect
      <em>human values</em> (ACM: primary human value was community wellbeing).
    </p>
    <h3 id="freeman-s-values-">Freeman&#39;s values:</h3>
    <ol>
      <li>Human Welfare</li>
      <li>
        Ownership and Property: right to control and dispose pieces of
        information.
      </li>
      <li>
        Privacy: consent/control over your personal information. Your right to
        how much people can be informed upon your life.
      </li>
      <li>
        Freedom from bias: eliminating unfairness as a result from your
        membership of a certain group.
      </li>
      <li>
        Universal Usability: (connection to freedom from bias), how to permit
        and include people with disabilities/financially worse off with
        technology.
      </li>
      <li>
        Trust: people need to be able to trust the products/services they use.
      </li>
      <li>
        Autonomy: refering to the human capacity to choose for itself.
        (Autocratic - tells you what to decide, limits user&#39;s choice for
        action)
      </li>
      <li>
        Informed Consent: fully aware what you are permitting, whilst having
        certain capacities (e.g over 12/13).
      </li>
      <li>
        Accountability: accountability party is the party that is ultimately
        accountability (aside from responsibility - e.g. managing construction
        worker who hurt themselves, you are accountability for their training).
      </li>
      <li>Courtesy</li>
      <li>
        Identity: respect people&#39;s identity (e.g. gender-neutral language,
        cannot assume financial status/class-identity, racial etc.), assume
        diversity: requires research on the diversity of the community the
        product/service wishes to cater to.
      </li>
      <li>Calmness: refering to not agitating users needlessly.</li>
      <li>Environmental: environmental sustainability.</li>
    </ol>
    <h3 id="ethical-legal-consequences-cont-">
      Ethical/Legal Consequences (cont...)
    </h3>
    <p>
      Safe + Beneficial (e.g. Welfare, Universal Usability, eliminate bias,
      Privacy could be at stake). Morally relevant decision making (some moral
      theories are seen better suited for certain situations), e.g. healthcare
      software: consequentialist moral theory will predominate. The type of
      moral thinking the artifical entity should be engaged depends on the
      context they are shaped/used in.
    </p>
    <h3 id="assessment">Assessment</h3>
    <p>
      Human Rights Commision on AI, and you must engage with the majority of
      human values discussed above. Code of Ethics for the Computer Society: 8
      principles. Australian AI Ethics Framework/Australia AI Ethics: 8
      principles.
    </p>
    <p>Use these to assemble your conceptual investigation.</p>
    <h3 id="artificial-moral-agents-readings-for-week5-">
      Artificial Moral Agents (readings for Week5)
    </h3>
    <p>
      Intersection of Cognitive Science and Philosophy: adopt a functionalist
      theory of mind. For example: brain injury, insert artifical hardware that
      performs the same function as that part of the brain. You wouldn&#39;t
      care if a machine is doing that specific function as long as you are
      functioning the same.
    </p>
    <p>Functionalist theory of mind:</p>
    <ul>
      <li>
        Map out various functions of the brain in terms of input and output.
      </li>
      <li>
        Moral personhood: software/hardware that has the capacity to attribute
        to a moral person as long as it functions correctly (e.g. sentience,
        memory, rationality).
      </li>
    </ul>

    <!-- JavaScript Bundle with Popper -->
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-u1OknCvxWvY5kfmNBILK2hRnQC3Pr17a+RTT6rIHI7NnikvbZlHgTPOOmMi466C8"
      crossorigin="anonymous"
    ></script>
  </body>
</html>
